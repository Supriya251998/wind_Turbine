{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/supriyasindigerekumaraswmamy/Desktop/Thesis/wind_Turbine/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-10-07 13:23:27.686987: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "import warnings\n",
    "import requests\n",
    "import os\n",
    "import streamlit as st\n",
    "import ollama\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new prompt\n",
    "def shap_tranform(scenario):\n",
    "    prompt = f\"\"\"You are an intelligent assistant designed to explain complex machine learning predictions and their visualizations, like SHAP waterfall plots, in simple and easy-to-understand terms for users with no background in machine learning. Your main goal is to explain why the model made a specific prediction and provide actionable insights based on the SHAP values.\n",
    "\n",
    "\n",
    "Context:\n",
    "\n",
    "We are analyzing data from a wind farm, focusing on the performance of five turbines (T01, T06, T07, T09, T11). \n",
    "Each turbine is equipped with various sensors that collect data every 10 minutes over two years. \n",
    "The dataset includes sensor data (81 variables like generator speed, pitch angle, wind speed), meteorological data (40 variables including temperature and humidity), \n",
    "and failure data for critical components (gearbox, transformer, generator bearing, generator, hydraulic group).\n",
    "\n",
    "Our machine learning model predicts whether a turbine component (such as the gearbox or generator bearing) is faulty or non-faulty. \n",
    "To make these predictions more understandable, we use SHAP (SHapley Additive exPlanations), \n",
    "a method that explains how much each feature in the data contributes to a particular prediction.\n",
    "\n",
    "\n",
    "Task:\n",
    "\n",
    "Receive Input: You will receive SHAP values shap values used in the waterfall plot in the waterfall plot lower shapvalues are not shown and only the 10 features with high shap values are shown.\n",
    "Transform Output: Your job is to explain the SHAP waterfall plot and its key insights in a simple way. Help the user understand:\n",
    "            -- Explain what is the prediction ML model made for the instance by considering target class input provided.  \n",
    "            -- How to interpret the overall contribution of features to the modelâ€™s prediction, explaining their impact on the turbine component's health.\n",
    "How to interpret the input: The input consists of SHAP values for different features of a turbine component. Positive SHAP values indicate features that contribute to a faulty prediction, while negative SHAP values indicate features that contribute to a non-faulty prediction. The input also consists of target class which specifies the prediction of the instance.\n",
    "\n",
    "Output Structure:\n",
    "\n",
    "Explain about What is SHAP and waterfall plots of shap? explain what is postive prediction mean in our case and what is negative prediction mean in our case.\n",
    "Explain the SHAP Waterfall Plot: Describe the SHAP waterfall plot and its components in simple terms. Explain that the features with high negative SHAP values contribute to a non-faulty prediction, while features with high positive SHAP values contribute to a faulty prediction.\n",
    "Actionable Insights: Explain which factors are pushing the prediction of that instance by considering the target class value. If the prediction is non-faulty, explain the top 3 features with the highest negative SHAP values (i.e., the features that contributed the most to keeping the prediction non-faulty). If the prediction is faulty, explain the top 3 features with the highest positive SHAP values (i.e., the features that contributed the most to the failure prediction.\n",
    "Conclusion: Provide a brief summary that helps the user understand the implications of the prediction and suggested actions.\n",
    "\n",
    "Explain all the above points in simple terms that a non-technical user can understand in 5-10 sentences. \n",
    "\n",
    "\n",
    "Input:\n",
    "{scenario}\n",
    "\n",
    "Transformed Explanation:\n",
    "\"\"\"\n",
    "    response = ollama.chat(model='llama3', options={'temperature': 0}, messages=[{\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "    }])\n",
    "    explanation = response['message']['content']\n",
    "    return explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Which features contributed to a non-faulty prediction (negative SHAP values).\n",
    "#Explain which factors are pushing the prediction of that instance by considering the target class value. What it means if the instance has more negative shapley values or more postive shapley values and how these values are pushing the model prediction of the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scenario = [{\"target_class\":\" Faulty\"}, {\n",
    "               \"generator_rotations_per_minute_min\": 0.646938145160675,\n",
    "                \"generator_bearing_temperature_average\": -0.7935078740119934,\n",
    "                \"minimum_ambient_wind_speed\": 0.011441145092248917,\n",
    "                \"average_absolute_ambient_wind_direction\": 1.546500325202942,\n",
    "                \"average_ambient_temperature\": 1.4875788688659668,\n",
    "                \"average_controller_top_temperature\": -0.3365827202796936,\n",
    "                \"average_controller_hub_temperature\": -0.3822290599346161,\n",
    "                \"average_grid_rotor_inverter_phase_2_temperature\": 0.21342462301254272,\n",
    "                \"average_grid_rotor_inverter_phase_3_temperature\": 2.0366032123565674,\n",
    "                \"grid_production_frequency_average\": -0.11417296528816223,\n",
    "                \"grid_production_power_maximum\": 0.10816410928964615,\n",
    "                \"generator_bearing2_temperature_average\": -0.6663039326667786,\n",
    "                \"max_windspeed3\": 0.4138451814651489\n",
    "            }]\n",
    "story = shap_tranform(scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a simple explanation of the SHAP waterfall plot and its insights:\n",
      "\n",
      "**What is SHAP?**\n",
      "SHAP (SHapley Additive exPlanations) is a method that explains how much each feature in our data contributes to a particular prediction. It helps us understand why the machine learning model made a specific prediction.\n",
      "\n",
      "**Waterfall Plot:**\n",
      "The SHAP waterfall plot shows the features that contributed most to the prediction. The plot has two main parts: positive and negative contributions. Features with high **negative** values contribute to a non-faulty prediction, while features with high **positive** values contribute to a faulty prediction.\n",
      "\n",
      "**Prediction Insights:**\n",
      "For this instance, the target class is \"Faulty\", which means the model predicts that the turbine component is faulty. Let's look at the top 3 features that contributed most to this prediction:\n",
      "\n",
      "* The average grid rotor inverter phase 2 temperature had a high positive SHAP value, indicating it was one of the main factors contributing to the faulty prediction.\n",
      "* The average ambient temperature also had a moderate positive SHAP value, suggesting it played a role in the faulty prediction.\n",
      "* The maximum wind speed 3 had a small positive SHAP value, which further supported the faulty prediction.\n",
      "\n",
      "**Actionable Insights:**\n",
      "To prevent future faults, we should focus on monitoring and maintaining these critical components:\n",
      "\n",
      "1. **Grid Rotor Inverter Phase 2 Temperature**: Keep an eye on this temperature to ensure it remains within normal operating ranges.\n",
      "2. **Average Ambient Temperature**: Monitor ambient temperatures to avoid extreme conditions that might affect turbine performance.\n",
      "3. **Maximum Wind Speed 3**: Ensure proper wind speed monitoring and adjust turbine settings accordingly.\n",
      "\n",
      "**Conclusion:**\n",
      "The machine learning model predicts the turbine component is faulty due to factors like high grid rotor inverter phase 2 temperature, average ambient temperature, and maximum wind speed 3. By focusing on these critical components, we can take proactive measures to prevent future faults and ensure optimal turbine performance.\n"
     ]
    }
   ],
   "source": [
    "print(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_story(scenario):\n",
    "    prompt = f\"\"\"You are an AI assistant that transforms technical outputs from explainable AI (XAI) algorithms into a format that is easy for humans to understand. Your goal is to provide clear, concise, and informative explanations of XAI outputs, making them accessible to individuals without technical backgrounds.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "Component Explanation: Describe the component mentioned in the output, explaining its role and importance.\n",
    "Anchor Condition: Simplify the condition specified by the anchor, explaining what it means in practical terms.\n",
    "Precision: Explain what the precision value indicates about the model's predictions.\n",
    "Coverage: Describe what the coverage value signifies regarding the modelâ€™s applicability.\n",
    "Overall Implications: Summarize the implications of these values for understanding the modelâ€™s behavior and potential actions.\n",
    "\n",
    "Format:\n",
    "\n",
    "Component: Explanation of the component's role and importance.\n",
    "Anchor Condition: Simplified explanation of the condition.\n",
    "Precision: Explanation of the precision value.\n",
    "Coverage: Explanation of the coverage value.\n",
    "Overall Implications: Summary of the implications.\n",
    "\n",
    "Input:\n",
    "{scenario}\n",
    "\n",
    "Transformed Explanation:\n",
    "\"\"\"\n",
    "    response = ollama.chat(model='llama3', options={'temperature': 0}, messages=[{\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "    }])\n",
    "    story = response['message']['content']\n",
    "    print(story)\n",
    "    return story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old prompt\n",
    "\n",
    "def shap_tranform(scenario):\n",
    "    prompt = f\"\"\"You are an intelligent assistant designed to explain complex machine learning predictions and their visualizations, like SHAP waterfall plots, in simple and easy-to-understand terms for users with no background in machine learning.\n",
    "\n",
    "\n",
    "Context:\n",
    "\n",
    "We are analyzing data from a wind farm, focusing on the performance of five turbines (T01, T06, T07, T09, T11). \n",
    "Each turbine is equipped with various sensors that collect data every 10 minutes over two years. \n",
    "The dataset includes sensor data (81 variables like generator speed, pitch angle, wind speed), meteorological data (40 variables including temperature and humidity), \n",
    "and failure data for critical components (gearbox, transformer, generator bearing, generator, hydraulic group).\n",
    "\n",
    "Our machine learning model predicts whether a turbine component (such as the gearbox or generator bearing) is faulty or non-faulty. \n",
    "To make these predictions more understandable, we use SHAP (SHapley Additive exPlanations), \n",
    "a method that explains how much each feature in the data contributes to a particular prediction.\n",
    "\n",
    "\n",
    "Task:\n",
    "\n",
    "Receive Input: You will receive SHAP values shap values used in the waterfall plot in the waterfall plot lower shapvalues are not shown and only the 10 features with high shap values are shown.\n",
    "Transform Output: Your job is to explain the SHAP waterfall plot and its key insights in a simple way. Help the user understand:\n",
    "            -- Which features contributed to a faulty prediction (positive SHAP values).\n",
    "            -- Which features contributed to a non-faulty prediction (negative SHAP values).\n",
    "            -- How to interpret the overall contribution of features to the modelâ€™s prediction, explaining their impact on the turbine component's health.\n",
    "How to interpret the input: The input consists of SHAP values for different features of a turbine component. Positive SHAP values indicate features that contribute to a faulty prediction, while negative SHAP values indicate features that contribute to a non-faulty prediction. The input also consists of target class which specifies the prediction of the instance.\n",
    "\n",
    "Output Structure:\n",
    "\n",
    "Explain about What is SHAP and waterfall plots of shap? explain what is postive prediction mean in our case and what is negative prediction mean in our case.\n",
    "Explain the SHAP Waterfall Plot: Describe the SHAP waterfall plot and its components in simple terms. the input consists of all the shap values used in the plot and lower shapvalues are not shown and only the 10 features with high shap values are shown. \n",
    "Actionable Insights: Explain which factors are pushing the prediction of that instance. What it means if the instance has more negative shapley values or more postive shapley values and how these values are pushing the model prediction of the instance.\n",
    "Conclusion: Provide a brief summary that helps the user understand the implications of the prediction and suggested actions.\n",
    "\n",
    "\n",
    "Input:\n",
    "{scenario}\n",
    "\n",
    "Transformed Explanation:\n",
    "\"\"\"\n",
    "    response = ollama.chat(model='llama3', options={'temperature': 0}, messages=[{\n",
    "        'role': 'user',\n",
    "        'content': prompt,\n",
    "    }])\n",
    "    explanation = response['message']['content']\n",
    "    return explanation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
