{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.model_selection import  train_test_split\n",
    "import copy\n",
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from xgboost import XGBClassifier as xgb\n",
    "from sklearn.model_selection import learning_curve, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "import mlflow\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "failures = pd.read_csv('./data/model_data/failures.csv',sep=',')\n",
    "components = failures['Component'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "for component in components:\n",
    "    globals()[f\"{component}_df\"] = pd.read_csv(f'./data/model_data/labelled_data_{component}.csv',sep=',')\n",
    "    globals()[f\"{component}_df\"]['Turbine_ID'] = encoder.fit_transform(['Turbine_ID']*globals()[f\"{component}_df\"].shape[0])\n",
    "    # set the date as the index\n",
    "    globals()[f\"{component}_df\"] = globals()[f\"{component}_df\"].set_index('Timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = GEARBOX_df.drop(columns=['Component'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_target_name = \"Failure (Target)\"\n",
    "for component in components:\n",
    "    X = globals()[f\"{component}_df\"].drop(columns=['Component',class_target_name])\n",
    "    y = globals()[f\"{component}_df\"][class_target_name]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    globals()[f\"{component}_X_train\"] = X_train\n",
    "    globals()[f\"{component}_X_test\"] = X_test\n",
    "    globals()[f\"{component}_y_train\"] = y_train\n",
    "    globals()[f\"{component}_y_test\"] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_experiment(experiment_name):\n",
    "    \"\"\"\n",
    "    Retrieve the ID of an existing MLflow experiment or create a new one if it doesn't exist.\n",
    "\n",
    "    This function checks if an experiment with the given name exists within MLflow.\n",
    "    If it does, the function returns its ID. If not, it creates a new experiment\n",
    "    with the provided name and returns its ID.\n",
    "\n",
    "    Parameters:\n",
    "    - experiment_name (str): Name of the MLflow experiment.\n",
    "\n",
    "    Returns:\n",
    "    - str: ID of the existing or newly created MLflow experiment.\n",
    "    \"\"\"\n",
    "\n",
    "    if experiment := mlflow.get_experiment_by_name(experiment_name):\n",
    "        return experiment.experiment_id\n",
    "    else:\n",
    "        return mlflow.create_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_id = get_or_create_experiment(\"Wind Turbine\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: review the links mentioned above for guidance on connecting to a managed tracking server, such as the free Databricks Community Edition\n",
    "\n",
    "mlflow.set_tracking_uri(\"http://localhost:8080\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# override Optuna's default logging to ERROR only\n",
    "optuna.logging.set_verbosity(optuna.logging.ERROR)\n",
    "\n",
    "# define a logging callback that will report on only new challenger parameter configurations if a\n",
    "# trial has usurped the state of 'best conditions'\n",
    "\n",
    "\n",
    "def champion_callback(study, frozen_trial):\n",
    "    \"\"\"\n",
    "    Logging callback that will report when a new trial iteration improves upon existing\n",
    "    best trial values.\n",
    "\n",
    "    Note: This callback is not intended for use in distributed computing systems such as Spark\n",
    "    or Ray due to the micro-batch iterative implementation for distributing trials to a cluster's\n",
    "    workers or agents.\n",
    "    The race conditions with file system state management for distributed trials will render\n",
    "    inconsistent values with this callback.\n",
    "    \"\"\"\n",
    "\n",
    "    winner = study.user_attrs.get(\"winner\", None)\n",
    "\n",
    "    if study.best_value and winner != study.best_value:\n",
    "        study.set_user_attr(\"winner\", study.best_value)\n",
    "        if winner:\n",
    "            improvement_percent = (abs(winner - study.best_value) / study.best_value) * 100\n",
    "            print(\n",
    "                f\"Trial {frozen_trial.number} achieved value: {frozen_trial.value} with \"\n",
    "                f\"{improvement_percent: .4f}% improvement\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Initial trial {frozen_trial.number} achieved value: {frozen_trial.value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    with mlflow.start_run(nested=True):\n",
    "        # Define hyperparameters\n",
    "        params = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'use_label_encoder': False,\n",
    "        'n_estimators': trial.suggest_categorical('n_estimators', [50, 100, 200]),\n",
    "        'max_depth': trial.suggest_categorical('max_depth', [10, 20, 40, 80]),\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.2),\n",
    "        'subsample': 1.0,\n",
    "        'colsample_bytree': 1.0,\n",
    "        'lambda': 1.0,\n",
    "        'alpha': 1.0,\n",
    "        'max_features': trial.suggest_int('max_features', 5, 104)  # Add max_features parameter\n",
    "    }\n",
    "        # Train XGBoost model\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(GEARBOX_X_train, GEARBOX_y_train)\n",
    "\n",
    "        selector = SelectFromModel(model, threshold=-np.inf, prefit=True, max_features=params['max_features'])\n",
    "        X_train_selected = selector.transform(X_train)\n",
    "        X_test_selected = selector.transform(X_test)\n",
    "        model.fit(X_train_selected, y_train)\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "       \n",
    "   \n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric('f1_weighted', f1)\n",
    "\n",
    "    return f1\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_name = \"first_attempt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(experiment_id=experiment_id, run_name=run_name, nested=True):\n",
    "    # Initialize the Optuna study\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=500,callbacks=[champion_callback])\n",
    "    mlflow.log_params(study.best_params)\n",
    "    mlflow.log_metrics({'f1_score': study.best_trial.value})\n",
    "\n",
    "    mlflow.set_tags(\n",
    "        tags={\n",
    "            \"project\": \"Wind Turbine\",\n",
    "            \"optimizer_engine\": \"optuna\",\n",
    "            \"model_family\": \"xgboost\",\n",
    "            \"feature_set_version\": 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    best_params = trial.params\n",
    "    best_model = XGBClassifier(**best_params)\n",
    "    best_model.fit(GEARBOX_X_train, GEARBOX_y_train)\n",
    "    selector = SelectFromModel(best_model, threshold=-np.inf, prefit=True, max_features=best_params['max_features'])\n",
    "    X_train_selected = selector.transform(X_train)\n",
    "    X_test_selected = selector.transform(X_test)\n",
    "    best_model.fit(X_train_selected, y_train)\n",
    "\n",
    "    artifact_path = \"model\"\n",
    "\n",
    "    mlflow.xgboost.log_model(\n",
    "        xgb_model=best_model,\n",
    "        artifact_path=artifact_path,\n",
    "        input_example= GEARBOX_X_train.iloc[[0]],\n",
    "        model_format=\"ubj\",\n",
    "        metadata={\"model_data_version\": 1},\n",
    "    )\n",
    "\n",
    "    model_uri = mlflow.get_artifact_uri(artifact_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
