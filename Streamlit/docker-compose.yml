version: '3'
services:
  app:
    build: .
    ports:
      - "80:80"  # MLflow
      - "8501:8501"  # Streamlit
    environment:
      - OLLAMA_HOST=http://ollama:11434
    depends_on:
      - ollama
    volumes:
      - ./mlruns:/app/mlruns
      - /Users/supriyasindigerekumaraswmamy/Desktop/Thesis/wind_Turbine/data:/app/data
      - /Users/supriyasindigerekumaraswmamy/Desktop/Thesis/wind_Turbine/utils:/app/utils
      - /Users/supriyasindigerekumaraswmamy/Desktop/Thesis/wind_Turbine/models:/app/models
      - /Users/supriyasindigerekumaraswmamy/Desktop/Thesis/wind_Turbine/xai/JSON:/app/xai/JSON
      
  ollama:
    image: ollama/ollama:latest
    ports:
        - 11434:11434
    volumes:
        - ./ollama/ollama:/root/.ollama
        - ./entrypoint.sh:/entrypoint.sh
    container_name: ollama
    pull_policy: always
    tty: true
    restart: always
    entrypoint: ["/usr/bin/bash", "/entrypoint.sh"]